"""
LATS Agent èŠ‚ç‚¹ - ç®€åŒ–ä¼˜åŒ–ç‰ˆæœ¬

ä¼˜åŒ– LATS Agent æ ¸å¿ƒé€»è¾‘ï¼Œæå‡ä»£ç å¯è¯»æ€§å’Œæ€§èƒ½
å‡å°‘å†—ä½™ä»£ç ï¼Œä¿æŒæ ¸å¿ƒåŠŸèƒ½çš„å®Œæ•´æ€§
"""
from collections import defaultdict
from typing import List, Tuple

from langchain_core.messages import AIMessage, BaseMessage
from langchain_core.output_parsers import JsonOutputToolsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import ToolNode
from sanic.log import logger

from src.core.agent.lats_agent.lats_agent_state import LatsAgentState, Node, Reflection
from src.core.llm.node.tools_node import ToolsNodes
from src.core.sanic_plus.utils.template_loader import TemplateLoader


class LatsAgentNode(ToolsNodes):
    """LATS Agent èŠ‚ç‚¹å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""

    # æ ¸å¿ƒé…ç½®
    MAX_CANDIDATES = 5
    MAX_TREE_HEIGHT = 5
    EXPLORATION_WEIGHT = 1.0

    def get_reflection_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–åæ€è¯„ä¼°é“¾"""
        async def reflection_chain_async(inputs):
            llm = self.get_llm_client(
                config["configurable"]["graph_request"], disable_stream=True)

            system_message = TemplateLoader.render_template(
                "prompts/lats_agent/reflection_evaluation")
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_message),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="candidate"),
            ])

            result = await self.call_with_structured_output(
                llm=llm, prompt=prompt, pydantic_model=Reflection, messages=inputs
            )
            return result

        return reflection_chain_async

    def get_expansion_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–å€™é€‰ç”Ÿæˆé“¾"""
        def generate_candidates(messages) -> List[BaseMessage]:
            llm = self.get_llm_client(
                config["configurable"]["graph_request"], disable_stream=True)
            bound_kwargs = llm.bind_tools(tools=self.tools).kwargs

            candidates = []
            logger.debug(f"ç”Ÿæˆ {self.MAX_CANDIDATES} ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆ")

            for i in range(self.MAX_CANDIDATES):
                chat_result = llm.generate(
                    [messages.to_messages()],
                    callbacks=[],
                    run_name=f"GenerateCandidate_{i + 1}",
                    **bound_kwargs,
                )
                candidate = chat_result.generations[0][0].message
                candidates.append(candidate)

                # ç»Ÿè®¡ token ä½¿ç”¨
                if hasattr(candidate, 'usage_metadata'):
                    self.tools_prompt_tokens += candidate.usage_metadata.get(
                        'input_tokens', 0)
                    self.tools_completions_tokens += candidate.usage_metadata.get(
                        'output_tokens', 0)

            return candidates

        system_message = TemplateLoader.render_template(
            "prompts/lats_agent/candidate_generation")
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ])

        return prompt_template | generate_candidates

    def select(self, root: Node) -> Node:
        """ä½¿ç”¨ UCB ç®—æ³•é€‰æ‹©æœ€ä½³èŠ‚ç‚¹"""
        if not root.children:
            return root

        node = root
        while node.children:
            max_child = max(
                node.children,
                key=lambda child: child.upper_confidence_bound(
                    self.EXPLORATION_WEIGHT)
            )
            node = max_child

        logger.debug(f"é€‰æ‹©æ·±åº¦ä¸º {node.depth} çš„èŠ‚ç‚¹")
        return node

    async def _process_candidates(
        self,
        candidates: List[BaseMessage],
        state: LatsAgentState,
        config: RunnableConfig
    ) -> Tuple[List[List[BaseMessage]], List[Reflection]]:
        """å¤„ç†å€™é€‰æ–¹æ¡ˆï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨å’Œè¯„ä¼°"""
        # è§£æå·¥å…·è°ƒç”¨
        parser = JsonOutputToolsParser(return_id=True)
        parsed_tool_calls = parser.batch(candidates)

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_node = ToolNode(self.tools, handle_tool_errors=True)
        collected_responses = defaultdict(list)

        for candidate_idx, tool_calls in enumerate(parsed_tool_calls):
            for tool_call in tool_calls:
                try:
                    response = tool_node.invoke({
                        "messages": [AIMessage(
                            content="",
                            tool_calls=[{
                                "name": tool_call["type"],
                                "args": tool_call["args"],
                                "id": tool_call["id"],
                            }]
                        )]
                    })
                    collected_responses[candidate_idx].append(
                        response["messages"][0])
                except Exception as e:
                    logger.warning(f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_call['type']}, é”™è¯¯: {e}")
                    collected_responses[candidate_idx].append(
                        AIMessage(content="å·¥å…·è°ƒç”¨å¤±è´¥"))

        # ç»„åˆæ¶ˆæ¯
        output_messages = []
        for idx, candidate in enumerate(candidates):
            output_messages.append([candidate] + collected_responses[idx])

        # åæ€è¯„ä¼°
        user_message = config["configurable"]["graph_request"].user_message
        reflection_func = self.get_reflection_chain(state, config)

        import asyncio
        reflection_inputs = [
            {"input": user_message, "candidate": messages}
            for messages in output_messages
        ]
        reflections = await asyncio.gather(*[
            reflection_func(inputs) for inputs in reflection_inputs
        ])

        # è®°å½•è¯„ä¼°ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼‰
        self._log_evaluation_summary(reflections)

        # é«˜åˆ†ç›´æ¥æ ‡è®°ä¸ºè§£å†³æ–¹æ¡ˆ
        for reflection in reflections:
            if reflection.score >= 9:
                reflection.found_solution = True

        return output_messages, reflections

    def _log_evaluation_summary(self, reflections: List[Reflection]) -> None:
        """è®°å½•è¯„ä¼°æ‘˜è¦"""
        if not reflections:
            return

        max_score = max(r.score for r in reflections)
        solved_count = sum(1 for r in reflections if r.found_solution)
        avg_score = sum(r.score for r in reflections) / len(reflections)

        logger.info(
            f"ğŸ“Š è¯„ä¼°å®Œæˆ | å€™é€‰æ•°: {len(reflections)} | "
            f"æœ€é«˜åˆ†: {max_score}/10 | å¹³å‡åˆ†: {avg_score:.1f}/10 | "
            f"è§£å†³æ–¹æ¡ˆ: {solved_count}ä¸ª"
        )

    async def expand(self, state: LatsAgentState, config: RunnableConfig) -> LatsAgentState:
        """æ‰©å±•æœç´¢æ ‘"""
        logger.info("ğŸŒ³ å¼€å§‹æ‰©å±•æœç´¢æ ‘")

        root = state["root"]
        if not root:
            logger.error("æœç´¢æ ‘æ ¹èŠ‚ç‚¹æœªåˆå§‹åŒ–")
            return state

        # é€‰æ‹©æœ€ä½³å€™é€‰èŠ‚ç‚¹
        best_candidate = self.select(root)
        messages = best_candidate.get_trajectory()

        # ç”Ÿæˆæ–°å€™é€‰
        user_message = config["configurable"]["graph_request"].user_message
        new_candidates = self.get_expansion_chain(state, config).invoke({
            "input": user_message,
            "messages": messages
        })

        # å¤„ç†å€™é€‰å¹¶è¯„ä¼°
        output_messages, reflections = await self._process_candidates(
            new_candidates, state, config
        )

        # æ·»åŠ è¯„ä¼°ç»“æœåˆ°çŠ¶æ€
        state['evaluation_results'] = [
            {
                'index': i + 1,
                'score': r.score,
                'found_solution': r.found_solution,
                'reflections': r.reflections,
                'message_content': output_messages[i][-1].content if output_messages[i] else ""
            }
            for i, r in enumerate(reflections)
        ]

        # æ‰©å±•æœç´¢æ ‘
        child_nodes = [
            Node(cand, parent=best_candidate, reflection=reflection)
            for cand, reflection in zip(output_messages, reflections)
        ]
        best_candidate.children.extend(child_nodes)

        # æ£€æŸ¥è§£å†³æ–¹æ¡ˆ
        solution_nodes = [node for node, r in zip(
            child_nodes, reflections) if r.found_solution]
        if solution_nodes:
            best_solution = max(
                solution_nodes, key=lambda node: node.reflection.score)

            logger.info(f"ğŸ‰ æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ! è¯„åˆ†: {best_solution.reflection.score}/10")

            # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_answer = await self._generate_final_answer(best_solution, config)
            state["messages"].append(final_answer)
            root._is_solved = True
        else:
            # æ·»åŠ æœ€ä½³ä¸­é—´ç»“æœ
            if child_nodes:
                best_node = max(
                    child_nodes, key=lambda node: node.reflection.score)
                if best_node.reflection.score >= 7:
                    best_message = best_node.get_trajectory(
                        include_reflections=False)[-1]
                    state["messages"].append(best_message)
                    logger.info(
                        f"â­ æ·»åŠ é«˜è´¨é‡ä¸­é—´ç»“æœ (è¯„åˆ†: {best_node.reflection.score}/10)")

        return state

    async def generate_final_answer(self, state: LatsAgentState, config: RunnableConfig) -> dict:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹"""
        logger.info("ğŸ“ ç”Ÿæˆæœ€ç»ˆæ€»ç»“ç­”æ¡ˆ")

        root = state["root"]

        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = await self._generate_final_answer(root, config)

        # å°†æœ€ç»ˆç­”æ¡ˆæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        state["messages"].append(final_answer)

        logger.info("âœ… æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆ")

        return state

    async def _generate_final_answer(self, solution_node: Node, config: RunnableConfig) -> BaseMessage:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        llm = self.get_llm_client(config["configurable"]["graph_request"])

        system_message = TemplateLoader.render_template(
            "prompts/lats_agent/intelligent_assistant")
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ])

        final_solution = solution_node.get_trajectory(
            include_reflections=False)[-1]

        # å®‰å…¨åœ°æå–ç”¨æˆ·æ ¸å¿ƒé—®é¢˜ï¼Œè¿‡æ»¤æ•æ„Ÿç³»ç»ŸæŒ‡ä»¤
        user_question = config['configurable']['graph_request'].user_message

        question = TemplateLoader.render_template(
            "prompts/lats_agent/final_answer_synthesis",
            {
                "user_question": user_question,
                "solution_content": final_solution.content
            }
        )

        chain = prompt_template | llm
        return chain.invoke({"input": question})

    def should_continue(self, state: LatsAgentState) -> str:
        """å†³å®šæ˜¯å¦ç»§ç»­æœç´¢"""
        root = state["root"]

        if root.is_solved:
            logger.info("âœ… æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            return "generate_final_answer"

        if root.height > self.MAX_TREE_HEIGHT:
            logger.info(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§æœç´¢æ·±åº¦ ({self.MAX_TREE_HEIGHT})ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            return "generate_final_answer"

        return "expand"

    async def generate_initial_response(self, state: LatsAgentState, config: RunnableConfig) -> dict:
        """ç”Ÿæˆåˆå§‹å“åº”"""
        logger.info("ğŸŒ± ç”Ÿæˆåˆå§‹å“åº”")

        # è·å–åˆå§‹å›ç­”é“¾
        llm = self.get_llm_client(config["configurable"]["graph_request"])
        system_message = TemplateLoader.render_template(
            "prompts/lats_agent/intelligent_assistant")

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ])

        user_message = config["configurable"]["graph_request"].user_message
        initial_chain = prompt_template | llm.bind_tools(tools=self.tools)
        res = initial_chain.invoke({"input": user_message})

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        parser = JsonOutputToolsParser(return_id=True)
        parsed = parser.invoke(res)

        tool_node = ToolNode(self.tools)
        tool_responses = [
            tool_node.invoke({
                "messages": [AIMessage(
                    content="",
                    tool_calls=[
                        {"name": r["type"], "args": r["args"], "id": r["id"]}],
                )]
            })
            for r in parsed
        ]

        # åˆå¹¶æ¶ˆæ¯
        output_messages = [res] + [tr["messages"][0] for tr in tool_responses]

        # è¯„ä¼°åˆå§‹å“åº”
        reflection_func = self.get_reflection_chain(state, config)
        reflection = await reflection_func({
            "input": user_message,
            "candidate": output_messages
        })

        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = Node(output_messages, reflection=reflection)
        state['root'] = root

        logger.info(f"ğŸ“Š åˆå§‹å“åº”è¯„ä¼° | è¯„åˆ†: {reflection.score}/10")

        # å°†åˆå§‹è¯„ä¼°ç»“æœæ·»åŠ åˆ°çŠ¶æ€ä¸­ï¼Œç”¨äºæµå¼è¾“å‡º
        # è¿™ä¸ªæ•°æ®ä¼šä½œä¸ºç‹¬ç«‹çš„chunkè¢«æµå¼ä¼ è¾“
        state['initial_evaluation'] = {
            'score': reflection.score,
            'reflections': reflection.reflections,
            'found_solution': reflection.found_solution
        }

        # æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        if output_messages:
            state["messages"].append(output_messages[-1])

        return state
