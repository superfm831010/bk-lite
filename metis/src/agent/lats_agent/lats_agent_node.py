from collections import defaultdict
from typing import Dict, List, Tuple, TypedDict

from langchain_core.callbacks import StdOutCallbackHandler, StreamingStdOutCallbackHandler
from langchain_core.messages import AIMessage, BaseMessage, AIMessageChunk
from langchain_core.output_parsers import JsonOutputToolsParser, PydanticToolsParser
from langchain_core.prompt_values import ChatPromptValue
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig
from langchain_core.runnables import chain as as_runnable
from langgraph.constants import END
from langgraph.prebuilt import ToolNode
from sanic.log import logger

from src.agent.lats_agent.lats_agent_state import LatsAgentState, Node, Reflection
from src.core.node.tools_node import ToolsNodes


class LatsAgentNode(ToolsNodes):
    # ç±»çº§å¸¸é‡é…ç½®
    MAX_CANDIDATES = 5  # æœ€å¤§å€™é€‰æ•°é‡
    MAX_TREE_HEIGHT = 5  # æœ€å¤§æ ‘é«˜åº¦
    EXPLORATION_WEIGHT = 1.0  # UCBç®—æ³•æ¢ç´¢æƒé‡

    def _process_tool_calls(self, message: AIMessage) -> List[Dict]:
        """å¤„ç†LLMè¿”å›çš„å·¥å…·è°ƒç”¨

        Args:
            message: åŒ…å«å·¥å…·è°ƒç”¨çš„AIæ¶ˆæ¯

        Returns:
            æ ¼å¼åŒ–çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        if not message.tool_calls:
            return []

        return [
            {
                "name": tool_call.name,
                "args": tool_call.args,
                "id": tool_call.id
            }
            for tool_call in message.tool_calls
        ]

    # ç§»é™¤å†—ä½™æ–¹æ³• _update_state_messagesï¼Œå› ä¸ºåœ¨ generate_initial_response å’Œ expand ä¸­å·²ç›´æ¥æ›´æ–° state['messages']

    def _create_tool_node(self) -> ToolNode:
        """åˆ›å»ºå·¥å…·èŠ‚ç‚¹å®ä¾‹

        Returns:
            å·¥å…·èŠ‚ç‚¹å®ä¾‹
        """
        return ToolNode(self.tools, handle_tool_errors=True)

    def get_reflection_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–ç”¨äºåæ€å’Œè¯„åˆ†å€™é€‰è§£å†³æ–¹æ¡ˆçš„é“¾

        åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆè´¨é‡çš„åæ€é“¾ã€‚è¯¥é“¾é€šè¿‡Reflectionå·¥å…·
        å¯¹è§£å†³æ–¹æ¡ˆè¿›è¡Œæ‰“åˆ†(0-10)ã€æä¾›åæ€æ€§è¯„ä»·ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦å®Œå…¨è§£å†³äº†ç”¨æˆ·é—®é¢˜ã€‚

        Args:
            state: å½“å‰æœç´¢çŠ¶æ€
            config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«è¯·æ±‚ä¿¡æ¯

        Returns:
            å¯æ‰§è¡Œçš„åæ€è¯„ä¼°é“¾
        """
        # è·å–é…ç½®çš„LLMå®¢æˆ·ç«¯
        llm = self.get_llm_client(config["configurable"]["graph_request"])

        # åˆ›å»ºåæ€æç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "å¯¹AIåŠ©æ‰‹çš„å›ç­”è¿›è¡Œåæ€å’Œè¯„åˆ†ã€‚è¯„ä¼°å›ç­”çš„å……åˆ†æ€§ã€å‡†ç¡®æ€§å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚",
                ),
                ("user", "{input}"),  # ç”¨æˆ·çš„åŸå§‹é—®é¢˜
                MessagesPlaceholder(variable_name="candidate"),  # å€™é€‰è§£å†³æ–¹æ¡ˆ
            ]
        )

        # æ„å»ºåæ€é“¾ï¼Œä½¿ç”¨Reflectionå·¥å…·å¼ºåˆ¶è¾“å‡ºç»“æ„åŒ–è¯„ä¼°
        reflection_llm_chain = (
                prompt
                | llm.bind_tools(
            tools=[Reflection],
            tool_choice="Reflection"  # å¼ºåˆ¶ä½¿ç”¨Reflectionå·¥å…·
        ).with_config(
            run_name="Reflection",  # ä¸ºè¿½è¸ªæ·»åŠ è¿è¡Œåç§°
            configurable={"verbose": False},  # ç¦æ­¢è¾“å‡ºåˆ°æ§åˆ¶å°
            callbacks=[]  # æ¸…ç©ºå›è°ƒï¼Œé˜²æ­¢è¾“å‡º
        )
                | PydanticToolsParser(tools=[Reflection])  # è§£ææˆReflectionå¯¹è±¡
        )

        logger.debug("åæ€è¯„ä¼°é“¾åˆ›å»ºå®Œæˆ")
        return reflection_llm_chain

    def get_expansion_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–ç”¨äºç”Ÿæˆå€™é€‰è§£å†³æ–¹æ¡ˆçš„é“¾

        Args:
            state: å½“å‰çŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            å€™é€‰è§£å†³æ–¹æ¡ˆç”Ÿæˆé“¾
        """

        def generate_candidates(messages: ChatPromptValue) -> List[BaseMessage]:
            """ç”Ÿæˆå¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆ

            Args:
                messages: è¾“å…¥æ¶ˆæ¯

            Returns:
                å€™é€‰è§£å†³æ–¹æ¡ˆæ¶ˆæ¯åˆ—è¡¨
            """
            llm = self.get_llm_client(config["configurable"]["graph_request"])
            bound_kwargs = llm.bind_tools(tools=self.tools).kwargs

            candidates = []
            logger.debug(f"å¼€å§‹ç”Ÿæˆ{self.MAX_CANDIDATES}ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆ")

            for i in range(self.MAX_CANDIDATES):
                chat_result = llm.generate(
                    [messages.to_messages()],
                    callbacks=[],
                    run_name=f"GenerateCandidate_{i + 1}",
                    **bound_kwargs,
                )
                candidate = chat_result.generations[0][0].message
                candidates.append(candidate)

                self.tools_prompt_tokens += candidate.usage_metadata['input_tokens']
                self.tools_completions_tokens += candidate.usage_metadata['output_tokens']

                logger.debug(f"å€™é€‰è§£å†³æ–¹æ¡ˆ #{i + 1}: {candidate}...")
                logger.debug(f"å€™é€‰è§£å†³æ–¹æ¡ˆ #{i + 1}: Tokenç”¨é‡:{candidate.usage_metadata}")

            return candidates

        prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are an AI assistant.",
                ),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="messages", optional=True),
            ]
        )

        expansion_chain = prompt_template | generate_candidates
        return expansion_chain

    def select(self, root: Node) -> Node:
        """ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œåœ¨æ¯ä¸ªæ ‘å±‚çº§é€‰æ‹©ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œç›´åˆ°è¾¾åˆ°å¶èŠ‚ç‚¹

        ä½¿ç”¨ä¸Šç½®ä¿¡ä»»è¾¹ç•Œç®—æ³•(UCB)æ¥é€‰æ‹©èŠ‚ç‚¹ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

        Args:
            root: æœç´¢æ ‘çš„æ ¹èŠ‚ç‚¹

        Returns:
            é€‰å®šçš„èŠ‚ç‚¹
        """
        if not root.children:
            logger.debug(f"æ ¹èŠ‚ç‚¹æ²¡æœ‰å­èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›æ ¹èŠ‚ç‚¹")
            return root

        node = root
        path = [node.depth]

        while node.children:
            max_child = max(
                node.children,
                key=lambda child: child.upper_confidence_bound(
                    self.EXPLORATION_WEIGHT)
            )
            node = max_child
            path.append(node.depth)

        logger.debug(f"ä»æ ¹èŠ‚ç‚¹é€‰æ‹©äº†è·¯å¾„: {path}ï¼Œæœ€ç»ˆé€‰æ‹©æ·±åº¦ä¸º{node.depth}çš„èŠ‚ç‚¹")
        return node

    def _process_candidates(
            self,
            candidates: List[BaseMessage],
            state: LatsAgentState,
            config: RunnableConfig
    ) -> Tuple[List[List[BaseMessage]], List[Reflection]]:
        """å¤„ç†å€™é€‰è§£å†³æ–¹æ¡ˆï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿›è¡Œåæ€è¯„ä¼°

        Args:
            candidates: å€™é€‰è§£å†³æ–¹æ¡ˆåˆ—è¡¨
            state: å½“å‰çŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            å¤„ç†åçš„æ¶ˆæ¯å’Œåæ€çš„å…ƒç»„
        """
        # è§£æå·¥å…·è°ƒç”¨
        parser = JsonOutputToolsParser(return_id=True)
        parsed_tool_calls = parser.batch(candidates)

        # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
        tool_node = self._create_tool_node()

        # æ‰å¹³åŒ–å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼Œä¿ç•™å€™é€‰ç´¢å¼•
        flattened_tool_calls = [
            (candidate_idx, tool_call)
            for candidate_idx, tool_calls in enumerate(parsed_tool_calls)
            for tool_call in tool_calls
        ]

        logger.debug(
            f"ä»{len(candidates)}ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆä¸­è§£æå‡º{len(flattened_tool_calls)}ä¸ªå·¥å…·è°ƒç”¨")

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_responses = []
        for candidate_idx, tool_call in flattened_tool_calls:
            try:
                response = tool_node.invoke({
                    "messages": [
                        AIMessage(
                            content="",
                            tool_calls=[{
                                "name": tool_call["type"],
                                "args": tool_call["args"],
                                "id": tool_call["id"],
                            }]
                        )
                    ]
                })
                tool_responses.append((candidate_idx, response))
                logger.debug(f"å·¥å…·è°ƒç”¨æˆåŠŸ: {tool_call['type']}")
            except Exception as e:
                logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_call['type']}, é”™è¯¯: {str(e)}")
                # å³ä½¿è°ƒç”¨å¤±è´¥ä¹Ÿæ·»åŠ ä¸€ä¸ªç©ºå“åº”ï¼Œä¿æŒç´¢å¼•ä¸€è‡´æ€§
                tool_responses.append(
                    (candidate_idx, {"messages": [AIMessage(content="å·¥å…·è°ƒç”¨å¤±è´¥")]}))

        # æŒ‰å€™é€‰ç´¢å¼•æ”¶é›†å·¥å…·å“åº”
        collected_responses = defaultdict(list)
        for candidate_idx, response in tool_responses:
            collected_responses[candidate_idx].append(response["messages"][0])

        # å°†å€™é€‰è§£å†³æ–¹æ¡ˆä¸å…¶å·¥å…·å“åº”ç»„åˆ
        output_messages = []
        for idx, candidate in enumerate(candidates):
            output_messages.append([candidate] + collected_responses[idx])

        # åˆ›å»ºåæ€é“¾è¿›è¡Œè¯„ä¼°
        @as_runnable
        def reflection_chain(inputs) -> Reflection:
            # åˆ›å»ºæ— è¾“å‡ºçš„é…ç½®
            silent_config = {
                "callbacks": [],  # æ¸…ç©ºå›è°ƒå‡½æ•°
                "configurable": {"verbose": False}  # è®¾ç½®ä¸ºéè¯¦ç»†æ¨¡å¼
            }

            # èåˆç°æœ‰é…ç½®ä¸é™é»˜é…ç½®
            invoke_config = {**config, **silent_config}

            # è°ƒç”¨åæ€é“¾ï¼Œä½†ç¦æ­¢è¾“å‡º
            tool_choices = self.get_reflection_chain(
                state, config).invoke(inputs, config=invoke_config)
            reflection = tool_choices[0]

            # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯AIæ¶ˆæ¯ï¼Œåˆ™æ— æ³•è§£å†³é—®é¢˜
            if not isinstance(inputs["candidate"][-1], AIMessage):
                reflection.found_solution = False
            return reflection

        # æ‰¹é‡è¯„ä¼°æ‰€æœ‰å€™é€‰è§£å†³æ–¹æ¡ˆ
        user_message = config["configurable"]["graph_request"].user_message
        reflection_inputs = [
            {"input": user_message, "candidate": messages}
            for messages in output_messages
        ]

        # åˆ›å»ºæ‰¹é‡å¤„ç†çš„é™é»˜é…ç½®
        batch_config = {**config, "callbacks": [], "configurable": {**
                                                                    (config.get("configurable") or {}),
                                                                    "verbose": False}}

        # ä½¿ç”¨é™é»˜é…ç½®æ‰§è¡Œæ‰¹é‡åæ€è¯„ä¼°
        reflections = reflection_chain.batch(
            reflection_inputs, config=batch_config)

        # è®°å½•åæ€ç»“æœï¼Œä½¿ç”¨ç´§å‡‘æ ¼å¼è¾“å‡ºå€™é€‰æ–¹æ¡ˆè¯„ä¼°ä¿¡æ¯
        summary_header = "ğŸ“Š å€™é€‰è§£å†³æ–¹æ¡ˆè¯„ä¼°ç»“æœæ±‡æ€»"
        header_border = "=" * 50
        table_headers = f"{'åºå·':^5} | {'è¯„åˆ†':^6} | {'æ˜¯å¦è§£å†³':^8} | {'è¯„ä¼°æ¦‚è¦'}"
        table_divider = "-" * 80

        # æ„å»ºè¯„ä¼°è¡¨æ ¼å†…å®¹
        rows = []
        for idx, reflection in enumerate(reflections):
            solution_status = "âœ…" if reflection.found_solution else "âŒ"
            # æˆªå–åæ€å†…å®¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ¦‚è¦ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
            summary = reflection.reflections[:50] + "..." if len(
                reflection.reflections) > 50 else reflection.reflections
            rows.append(f"{idx + 1:^5} | {reflection.score:^6}/10 | {solution_status:^8} | {summary}")

            # åœ¨DEBUGçº§åˆ«è¾“å‡ºå®Œæ•´çš„åæ€å†…å®¹
            logger.debug(f"å€™é€‰æ–¹æ¡ˆ #{idx + 1} å®Œæ•´è¯„ä¼°:\n{reflection.reflections}")

            # è¾“å‡ºæ–¹æ¡ˆå†…å®¹æ¦‚è¦
            if idx < len(output_messages) and output_messages[idx]:
                # è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆå›ç­”ï¼‰
                message_content = output_messages[idx][-1].content if output_messages[idx][-1].content else "æ— å†…å®¹"
                content_summary = message_content[:50] + "..." if len(
                    message_content) > 50 else message_content
                logger.debug(f"æ–¹æ¡ˆå†…å®¹: {content_summary}")

        # æ‰¾å‡ºæœ€é«˜è¯„åˆ†å’Œæ˜¯å¦æœ‰è§£å†³æ–¹æ¡ˆ
        summary_footer = ""
        if reflections:
            max_score = max(r.score for r in reflections)
            solved_count = sum(1 for r in reflections if r.found_solution)
            summary_footer = f"âœ¨ æœ€é«˜è¯„åˆ†: {max_score}/10, æ‰¾åˆ° {solved_count} ä¸ªè§£å†³æ–¹æ¡ˆ"

        # å°†æ‰€æœ‰å†…å®¹æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²è¿›è¡Œè¾“å‡º
        evaluation_summary = (
            f"\n{header_border}\n{summary_header}\n{header_border}\n"
            f"{table_headers}\n{table_divider}\n"
            f"{chr(10).join(rows)}\n{header_border}\n"
            f"{summary_footer if summary_footer else ''}"
        )

        # ä½¿ç”¨å•ä¸€æ—¥å¿—è¯­å¥è¾“å‡ºæ•´ä¸ªè¯„ä¼°æ±‡æ€»
        logger.info(evaluation_summary)

        # å‡å¦‚åˆ†æ•°ç­‰äº10ï¼Œåˆ™è®¤ä¸ºå·²ç»æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ
        for reflection in reflections:
            if reflection.score == 10:
                reflection.found_solution = True

        return output_messages, reflections

    def expand(self, state: LatsAgentState, config: RunnableConfig) -> LatsAgentState:
        """æ‰©å±•æœç´¢æ ‘ï¼Œç”Ÿæˆæ–°çš„å€™é€‰è§£å†³æ–¹æ¡ˆ

        Langgraph æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸»è¦æœç´¢æ­¥éª¤ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³å€™é€‰èŠ‚ç‚¹å¹¶ç”Ÿæˆæ–°çš„è§£å†³æ–¹æ¡ˆ

        Args:
            state: å½“å‰çŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("å¼€å§‹æ‰©å±•æœç´¢æ ‘...")

        # è·å–æœç´¢æ ‘æ ¹èŠ‚ç‚¹
        root = state["root"]
        if not root:
            logger.error("æœç´¢æ ‘æ ¹èŠ‚ç‚¹æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œæ‰©å±•")
            return state

        # è®°å½•å½“å‰çŠ¶æ€æŒ‡æ ‡
        logger.debug(
            f"å½“å‰æœç´¢æ ‘é«˜åº¦: {root.height}, èŠ‚ç‚¹æ•°é‡: {self._count_nodes(root)}")

        # ä½¿ç”¨æ ‘æœç´¢ç®—æ³•é€‰æ‹©å½“å‰æœ€ä½³å€™é€‰èŠ‚ç‚¹
        best_candidate = self.select(root)
        messages = best_candidate.get_trajectory()

        # è·å–ç”¨æˆ·åŸå§‹æ¶ˆæ¯
        user_message = config["configurable"]["graph_request"].user_message
        logger.debug(f"é€‰æ‹©æ·±åº¦ä¸º{best_candidate.depth}çš„å€™é€‰èŠ‚ç‚¹è¿›è¡Œæ‰©å±•")

        # ç”Ÿæˆæ–°çš„å€™é€‰è§£å†³æ–¹æ¡ˆ
        new_candidates = self.get_expansion_chain(state, config).invoke({
            "input": user_message,
            "messages": messages
        })
        logger.debug(f"æˆåŠŸç”Ÿæˆ{len(new_candidates)}ä¸ªæ–°å€™é€‰è§£å†³æ–¹æ¡ˆ")

        # å¤„ç†å€™é€‰è§£å†³æ–¹æ¡ˆå¹¶è·å–åæ€è¯„ä¼°
        output_messages, reflections = self._process_candidates(
            new_candidates, state, config
        )

        # ç»Ÿè®¡å·²åœ¨_process_candidatesæ–¹æ³•ä¸­å®Œæˆï¼Œè¿™é‡Œä¸å†é‡å¤è¾“å‡º

        # æ‰©å±•æœç´¢æ ‘ï¼Œæ·»åŠ å­èŠ‚ç‚¹
        child_nodes = [
            Node(cand, parent=best_candidate, reflection=reflection)
            for cand, reflection in zip(output_messages, reflections)
        ]

        logger.info(f"å°†{len(child_nodes)}ä¸ªæ–°èŠ‚ç‚¹æ·»åŠ åˆ°æœç´¢æ ‘")
        best_candidate.children.extend(child_nodes)

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ
        has_solution = any(r.found_solution for r in reflections)
        if has_solution:
            solution_nodes = [node for node, r in zip(
                child_nodes, reflections) if r.found_solution]
            best_solution = max(
                solution_nodes, key=lambda node: node.reflection.score)

            # æ¼‚äº®åœ°æ‰“å°è§£å†³æ–¹æ¡ˆä¿¡æ¯
            logger.info("=" * 50)
            logger.info("ğŸ‰ æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ!")
            logger.info("=" * 50)
            logger.info(
                f"è¯„åˆ†: {best_solution.reflection.score}/10 | æ·±åº¦: {best_solution.depth} | æœç´¢æ ‘èŠ‚ç‚¹æ•°: {self._count_nodes(root)}")

            # è·å–æœ€ä½³è§£å†³æ–¹æ¡ˆçš„å†…å®¹å¹¶æ·»åŠ åˆ°çŠ¶æ€æ¶ˆæ¯
            final_solution = best_solution.get_trajectory(
                include_reflections=False)[-1]
            logger.info(f"è§£å†³æ–¹æ¡ˆå†…å®¹: {final_solution}")

            # æ‰“å°åæ€è¯„ä¼°
            reflection_text = best_solution.reflection.reflections[:100] + "..." if len(
                best_solution.reflection.reflections) > 100 else best_solution.reflection.reflections
            logger.info(f"è¯„ä¼°æ¦‚è¦: {reflection_text}")
            logger.info("-" * 50)

            # æ›´æ–°çŠ¶æ€æ¶ˆæ¯
            logger.debug("æ›´æ–°çŠ¶æ€æ¶ˆæ¯ï¼Œæ·»åŠ æœ€ä½³è§£å†³æ–¹æ¡ˆ")

            llm = self.get_llm_client(config["configurable"]["graph_request"])
            prompt_template = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        "æ‚¨æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œè¯·å°½å¯èƒ½å‡†ç¡®ã€å…¨é¢åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚",
                    ),
                    ("user", "{input}"),  # ç”¨æˆ·è¾“å…¥
                    MessagesPlaceholder(variable_name="messages",
                                        optional=True),  # å¯é€‰çš„ä¸Šä¸‹æ–‡æ¶ˆæ¯
                ]
            )
            chain = prompt_template | llm
            question = f"ç”¨æˆ·çš„é—®é¢˜æ˜¯:{config['configurable']['graph_request'].user_message}"
            question += f"ç»è¿‡Lats Agentåˆ†æåï¼Œæ‰¾åˆ°çš„è§£å†³æ–¹æ¡ˆæ˜¯:{final_solution.content}"
            question += "è¯·ç»“åˆç”¨æˆ·çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œå›å¤ç”¨æˆ·çš„é—®é¢˜,å‡†ç¡®ï¼Œå…¨é¢ï¼Œç®€æ´ï¼Œä¸è¦æé€ äº‹å®ã€‚"
            msg = chain.invoke({
                "input": question
            })
            state["messages"].append(msg)

            # æ ‡è®°æ ¹èŠ‚ç‚¹ä¸ºå·²è§£å†³
            root._is_solved = True
        else:
            logger.info("ğŸ”„ æœ¬è½®æœç´¢æœªæ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œç»§ç»­æ¢ç´¢")

            # å³ä½¿æ²¡æœ‰æ‰¾åˆ°å®Œå…¨è§£å†³æ–¹æ¡ˆï¼Œä¹Ÿå¯ä»¥æ·»åŠ å½“å‰æœ€ä½³å›ç­”ä»¥æä¾›æ¸è¿›å¼å“åº”
            if child_nodes:
                best_node = max(
                    child_nodes, key=lambda node: node.reflection.score)
                if best_node.reflection.score >= 7:  # åªæœ‰å½“è¯„åˆ†è¶³å¤Ÿé«˜æ—¶æ‰æ›´æ–°
                    # æ¼‚äº®åœ°æ‰“å°é«˜è´¨é‡ä¸­é—´ç»“æœ
                    logger.info("=" * 50)
                    logger.info(
                        f"â­ æ·»åŠ é«˜è´¨é‡ä¸­é—´ç»“æœ (è¯„åˆ†: {best_node.reflection.score}/10)")
                    best_message = best_node.get_trajectory(
                        include_reflections=False)[-1]
                    content_summary = best_message.content[:100] + "..." if len(
                        best_message.content) > 100 else best_message.content
                    logger.info(f"å†…å®¹æ¦‚è¦: {content_summary}")
                    logger.info("-" * 50)

                    # æ›´æ–°çŠ¶æ€æ¶ˆæ¯
                    state["messages"].append(best_message)

        return state

    def _count_nodes(self, node: Node) -> int:
        """è®¡ç®—æœç´¢æ ‘ä¸­çš„èŠ‚ç‚¹æ€»æ•°

        Args:
            node: å¼€å§‹è®¡æ•°çš„èŠ‚ç‚¹

        Returns:
            èŠ‚ç‚¹æ€»æ•°
        """
        count = 1  # å½“å‰èŠ‚ç‚¹
        for child in node.children:
            count += self._count_nodes(child)
        return count

    def should_continue(self, state: LatsAgentState) -> TypedDict:
        """å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œå›¾ä¸­çš„ä¸‹ä¸€æ­¥

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°æˆ–ç»“æŸæ ‡è®°
        """
        root = state["root"]

        # è®°å½•å½“å‰æ‰§è¡ŒçŠ¶æ€çš„å…³é”®ä¿¡æ¯
        logger.debug(f"æœç´¢æ ‘é«˜åº¦: {root.height}, æ˜¯å¦è§£å†³: {root.is_solved}")

        # å¦‚æœæ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œç»“æŸæœç´¢
        if root.is_solved:
            logger.info("æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œç»“æŸæœç´¢")
            return END

        # å¦‚æœæœç´¢æ·±åº¦è¶…è¿‡é™åˆ¶ï¼Œç»“æŸæœç´¢
        if root.height > LatsAgentNode.MAX_TREE_HEIGHT:
            logger.info(f"æœç´¢æ·±åº¦è¾¾åˆ°ä¸Šé™ ({LatsAgentNode.MAX_TREE_HEIGHT})ï¼Œç»“æŸæœç´¢")
            return END

        # ç»§ç»­æ¢ç´¢
        return "expand"

    def get_initial_answer_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–ç”¨äºç”Ÿæˆåˆå§‹å›ç­”çš„é“¾

        åˆ›å»ºä¸€ä¸ªç”¨äºç”Ÿæˆæœç´¢æ ‘æ ¹èŠ‚ç‚¹åˆå§‹å›ç­”çš„é“¾ã€‚è¿™ä¸ªåˆå§‹å›ç­”ä½œä¸ºæœç´¢çš„èµ·ç‚¹ï¼Œ
        åç»­è¿­ä»£å°†åŸºäºæ­¤è¿›è¡Œæ”¹è¿›å’Œæ‰©å±•ã€‚

        Args:
            state: å½“å‰æœç´¢çŠ¶æ€
            config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«è¯·æ±‚ä¿¡æ¯

        Returns:
            å¯æ‰§è¡Œçš„åˆå§‹å›ç­”ç”Ÿæˆé“¾
        """
        # è·å–é…ç½®çš„LLMå®¢æˆ·ç«¯
        llm = self.get_llm_client(config["configurable"]["graph_request"])

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "æ‚¨æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œè¯·å°½å¯èƒ½å‡†ç¡®ã€å…¨é¢åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚",
                ),
                ("user", "{input}"),  # ç”¨æˆ·è¾“å…¥
                MessagesPlaceholder(variable_name="messages",
                                    optional=True),  # å¯é€‰çš„ä¸Šä¸‹æ–‡æ¶ˆæ¯
            ]
        )

        # æ„å»ºåˆå§‹å›ç­”é“¾ï¼Œç»‘å®šå·¥å…·ä»¥å…è®¸å·¥å…·è°ƒç”¨
        initial_answer_chain = prompt_template | llm.bind_tools(
            tools=self.tools  # å…è®¸æ¨¡å‹ä½¿ç”¨é…ç½®çš„å·¥å…·
        ).with_config(
            run_name="GenerateInitialCandidate"  # ä¸ºè¿½è¸ªæ·»åŠ è¿è¡Œåç§°
        )

        logger.debug("åˆå§‹å›ç­”ç”Ÿæˆé“¾åˆ›å»ºå®Œæˆ")
        return initial_answer_chain

    def generate_initial_response(self, state: LatsAgentState, config: RunnableConfig) -> dict:
        """ç”Ÿæˆåˆå§‹å“åº”å¹¶æ„å»ºæœç´¢æ ‘æ ¹èŠ‚ç‚¹

        Args:
            state: å½“å‰çŠ¶æ€
            config: è¿è¡Œé…ç½®

        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("å¼€å§‹ç”Ÿæˆåˆå§‹å“åº”...")

        # è·å–ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆåˆå§‹å›ç­”
        user_message = config["configurable"]["graph_request"].user_message
        res = self.get_initial_answer_chain(
            state, config).invoke({"input": user_message})
        logger.debug(f"åˆå§‹å›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(res.content)}")

        # è§£æå·¥å…·è°ƒç”¨
        parser = JsonOutputToolsParser(return_id=True)
        parsed = parser.invoke(res)

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_node = ToolNode(self.tools)
        tool_responses = [
            tool_node.invoke(
                {
                    "messages": [
                        AIMessage(
                            content="",
                            tool_calls=[
                                {"name": r["type"],
                                 "args": r["args"], "id": r["id"]}
                            ],
                        )
                    ]
                }
            )
            for r in parsed
        ]

        # åˆå¹¶æ¶ˆæ¯
        output_messages = [res] + [tr["messages"][0] for tr in tool_responses]

        # å¯¹åˆå§‹å›ç­”è¿›è¡Œåæ€è¯„ä¼°ï¼ˆä½¿ç”¨é™é»˜é…ç½®ï¼‰
        silent_config = {**config, "callbacks": [],
                         "configurable": {"verbose": False}}
        reflection = self.get_reflection_chain(state, config).invoke(
            {"input": user_message, "candidate": output_messages},
            config=silent_config
        )

        # åˆ›å»ºæœç´¢æ ‘æ ¹èŠ‚ç‚¹
        r = reflection[0]
        r.found_solution = False  # åˆå§‹å“åº”é€šå¸¸ä¸ç®—ä½œæœ€ç»ˆè§£å†³æ–¹æ¡ˆ
        root = Node(output_messages, reflection=r)

        # ä¼˜é›…åœ°è¾“å‡ºåˆå§‹å“åº”çš„è¯„ä¼°ä¿¡æ¯
        logger.info("=" * 50)
        logger.info("ğŸŒŸ åˆå§‹å“åº”è¯„ä¼°ç»“æœ")
        logger.info("=" * 50)
        solution_status = "âœ…" if r.found_solution else "âŒ"
        logger.info(f"è¯„åˆ†: {r.score}/10 | æ˜¯å¦è§£å†³é—®é¢˜: {solution_status}")
        logger.info(f"è¯„ä¼°æ¦‚è¦: {r.reflections}")

        # è®°å½•åˆå§‹è§£å†³æ–¹æ¡ˆå†…å®¹
        if output_messages:
            solution_content = root.get_trajectory(
                include_reflections=False)[-1].content
            content_summary = solution_content[:100] + "..." if len(
                solution_content) > 100 else solution_content
            logger.info(f"åˆå§‹è§£å†³æ–¹æ¡ˆ: {content_summary}")
            logger.info("-" * 50)

        # æ›´æ–°çŠ¶æ€
        state['root'] = root

        # å°†åˆå§‹å›ç­”æ·»åŠ åˆ°çŠ¶æ€çš„æ¶ˆæ¯åˆ—è¡¨
        if output_messages:
            final_message = output_messages[-1]
            logger.debug(
                f"å°†åˆå§‹å“åº”æ·»åŠ åˆ°state['messages']: {final_message.content[:100]}...")
            state["messages"].append(final_message)

        return state
