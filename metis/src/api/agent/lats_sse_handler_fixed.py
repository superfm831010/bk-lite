"""
LATS Agent SSE å¤„ç†å™¨ - ä¿®å¤ç‰ˆæœ¬

ä¸»è¦ä¿®å¤ï¼š
1. åŸºäº LATS èŠ‚ç‚¹çŠ¶æ€åˆ¤æ–­æœ€ç»ˆç­”æ¡ˆï¼Œè€Œä¸æ˜¯å†…å®¹åˆ†æ
2. å¤§å¹…å‡å°‘debugæ—¥å¿—è¾“å‡ºï¼Œæé«˜å¯è¯»æ€§
3. æ­£ç¡®å¤„ç†æœç´¢å®ŒæˆçŠ¶æ€
"""
import asyncio
import json
from typing import Dict, Any, Optional
from datetime import datetime

from sanic.log import logger


async def stream_lats_response(
    workflow,
    body: Dict[str, Any],
    chat_id: str,
    model: str,
    res
):
    """
    æµå¼å¤„ç† LATS Agent å“åº”
    åŸºäºèŠ‚ç‚¹ç±»å‹å’ŒçŠ¶æ€è¿›è¡Œæ™ºèƒ½è¾“å‡ºæ§åˆ¶
    """
    created = int(datetime.now().timestamp())
    sent_contents = set()  # ç”¨äºå»é‡
    iteration_counter = 0  # è¿­ä»£è®¡æ•°å™¨
    current_node_type = None  # å½“å‰èŠ‚ç‚¹ç±»å‹

    # ç”¨äºèšåˆæµå¼æ¶ˆæ¯
    streaming_buffer = ""
    last_node_with_stream = None

    lats_state = {
        'is_searching': False,
        'has_initial_response': False,
        'search_iterations': 0,
        'final_solution_found': False,
        'evaluation_shown': False
    }

    try:
        logger.info(f"[LATS SSE] å¼€å§‹æµå¼å¤„ç†ï¼Œchat_id: {chat_id}")

        # å‘é€æœç´¢å¼€å§‹æ¶ˆæ¯
        start_content = "ğŸ” **æ­£åœ¨å¯åŠ¨ LATS æ™ºèƒ½æœç´¢...**\n\nğŸ§  åˆå§‹åŒ–è¯­è¨€è¾…åŠ©æ ‘æœç´¢å¼•æ“\n\nğŸ’¡ å‡†å¤‡ç”Ÿæˆå¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆå¹¶è¿›è¡Œæ·±åº¦æœç´¢"
        await _send_sse_data(res, chat_id, created, model, start_content)
        sent_contents.add(start_content)
        await asyncio.sleep(0.3)

        # è·å–æµå¼è¿­ä»£å™¨
        stream_iter = await workflow.stream(body)

        async for chunk in stream_iter:
            # åªåœ¨å¿…è¦æ—¶è®°å½•chunkç±»å‹ï¼Œå‡å°‘æ—¥å¿—å™ªéŸ³
            chunk_type = type(chunk).__name__

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆçŠ¶æ€ï¼Œè¡¨ç¤ºæœç´¢å·²å®Œæˆ
            if isinstance(chunk, dict) and 'messages' in chunk and 'root' in chunk:
                # è¿™è¡¨ç¤ºæ˜¯æœ€ç»ˆçš„çŠ¶æ€è¾“å‡ºï¼ŒåŒ…å«æœç´¢ç»“æœ
                root_node = chunk.get('root')
                if root_node and hasattr(root_node, 'is_solved'):
                    if root_node.is_solved:
                        lats_state['final_solution_found'] = True
                        logger.info("[LATS SSE] æœç´¢å®Œæˆï¼šæ‰¾åˆ°è§£å†³æ–¹æ¡ˆ")
                    else:
                        logger.info("[LATS SSE] æœç´¢å®Œæˆï¼šæœªæ‰¾åˆ°å®Œç¾è§£å†³æ–¹æ¡ˆï¼Œè¿”å›æœ€ä½³å€™é€‰")

                # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆæ¶ˆæ¯éœ€è¦è¾“å‡º
                messages = chunk.get('messages', [])
                if messages:
                    final_message = messages[-1]
                    if hasattr(final_message, 'content') and final_message.content:
                        # åŸºäº LATS èŠ‚ç‚¹çŠ¶æ€å†³å®šå¦‚ä½•å±•ç¤ºæœ€ç»ˆç­”æ¡ˆ
                        if lats_state['final_solution_found']:
                            content = f"\n\nğŸ¯ **LATS æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆ**\n\n{final_message.content}\n\n"
                        else:
                            content = f"\n\nğŸ’¡ **LATS æœ€ä½³å€™é€‰ç­”æ¡ˆ**\n\n{final_message.content}\n\n"

                        if content not in sent_contents:
                            await _send_sse_data(res, chat_id, created, model, content)
                            sent_contents.add(content)
                            logger.info(f"[LATS SSE] å‘é€æœ€ç»ˆç­”æ¡ˆ")
                continue

            # å¤„ç†èŠ‚ç‚¹è½¬æ¢
            if isinstance(chunk, dict) and len(chunk) == 1:
                node_name = next(iter(chunk.keys()))

                # å¦‚æœæœ‰æœªå¤„ç†çš„æµå¼å†…å®¹ï¼Œå…ˆå¤„ç†å®ƒ
                if streaming_buffer and last_node_with_stream:
                    await _process_buffered_stream(
                        streaming_buffer, last_node_with_stream, lats_state,
                        iteration_counter, res, chat_id, created, model, sent_contents
                    )
                    streaming_buffer = ""
                    last_node_with_stream = None

                current_node_type = node_name
                logger.info(f"[LATS SSE] èŠ‚ç‚¹: {node_name}")

                # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ›´æ–°çŠ¶æ€å’Œå‘é€æ¶ˆæ¯
                await _handle_node_transition(
                    node_name, lats_state, iteration_counter,
                    res, chat_id, created, model, sent_contents
                )

                if node_name == "expand":
                    iteration_counter += 1
                    lats_state['search_iterations'] = iteration_counter
                continue

            # å¤„ç†æ¶ˆæ¯æµ
            if isinstance(chunk, (tuple, list)) and len(chunk) > 0:
                message = chunk[0]

                if message is None:
                    continue

                message_type = type(message).__name__

                # å¤„ç†æµå¼æ¶ˆæ¯èšåˆ
                if "Chunk" in message_type and hasattr(message, 'content'):
                    streaming_buffer += message.content
                    last_node_with_stream = current_node_type
                    continue

                # å¤„ç†å®Œæ•´æ¶ˆæ¯
                content = await _process_node_message(
                    message, current_node_type, lats_state, iteration_counter
                )

                if content and content not in sent_contents:
                    await _send_sse_data(res, chat_id, created, model, content)
                    sent_contents.add(content)
                    await _adaptive_delay(content)

        # å¤„ç†å‰©ä½™çš„æµå¼å†…å®¹
        if streaming_buffer and last_node_with_stream:
            await _process_buffered_stream(
                streaming_buffer, last_node_with_stream, lats_state,
                iteration_counter, res, chat_id, created, model, sent_contents
            )

        # å‘é€æœç´¢å®Œæˆæ¶ˆæ¯
        if lats_state['final_solution_found']:
            completion_content = "\n\n---\n\nâœ¨ **LATS æœç´¢æˆåŠŸå®Œæˆï¼**\n\nğŸ‰ å·²æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆ\n\nğŸ’« å¸Œæœ›æˆ‘çš„å›ç­”å¯¹æ‚¨æœ‰å¸®åŠ©"
        else:
            completion_content = "\n\n---\n\nâœ¨ **LATS æœç´¢å®Œæˆï¼**\n\nğŸ‰ å·²å®Œæˆæ·±åº¦æœç´¢å’Œå¤šå€™é€‰æ–¹æ¡ˆè¯„ä¼°\n\nğŸ’« åŸºäºå½“å‰æœ€ä½³æ–¹æ¡ˆä¸ºæ‚¨æä¾›å›ç­”"

        await _send_sse_data(res, chat_id, created, model, completion_content)

        # å‘é€ç»“æŸæ ‡å¿—
        await _send_end_signal(res, chat_id, created, model)
        logger.info(f"[LATS SSE] æµå¼å¤„ç†å®Œæˆï¼Œchat_id: {chat_id}")

    except Exception as e:
        logger.error(f"[LATS SSE] å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", exc_info=True)
        error_content = f"\n\n---\n\nâŒ **LATS æœç´¢è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜**\n\nğŸ”§ **é”™è¯¯è¯¦æƒ…ï¼š**\n{str(e)}\n\nğŸ’¡ **å»ºè®®ï¼š**\nè¯·ç¨åé‡è¯•"
        await _send_sse_data(res, chat_id, created, model, error_content, finish_reason="stop")


async def _process_buffered_stream(
    buffered_content: str,
    node_type: str,
    lats_state: Dict[str, Any],
    iteration_counter: int,
    res, chat_id: str, created: int, model: str,
    sent_contents: set
) -> None:
    """å¤„ç†èšåˆåçš„æµå¼å†…å®¹"""
    if not buffered_content.strip():
        return

    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ¶ˆæ¯å¯¹è±¡æ¥å¤„ç†èšåˆå†…å®¹
    class MockMessage:
        def __init__(self, content):
            self.content = content

    mock_message = MockMessage(buffered_content.strip())
    content = await _process_node_message(mock_message, node_type, lats_state, iteration_counter)

    if content and content not in sent_contents:
        await _send_sse_data(res, chat_id, created, model, content)
        sent_contents.add(content)
        await _adaptive_delay(content)


async def _handle_node_transition(
    node_name: str,
    lats_state: Dict[str, Any],
    iteration_counter: int,
    res, chat_id: str, created: int, model: str,
    sent_contents: set
) -> None:
    """å¤„ç†èŠ‚ç‚¹è½¬æ¢ï¼Œå‘é€é€‚å½“çš„çŠ¶æ€æ¶ˆæ¯"""

    node_messages = {
        "generate_initial_response": "\nğŸŒ± **ç”Ÿæˆåˆå§‹è§£å†³æ–¹æ¡ˆ**\n\nğŸ¯ åˆ†æé—®é¢˜å¹¶æ„å»ºç¬¬ä¸€ä¸ªå€™é€‰å›ç­”",
        "expand": f"\n\n---\n\nğŸŒ³ **æœç´¢è¿­ä»£ #{iteration_counter + 1}**\n\nğŸ” æ¢ç´¢æœç´¢æ ‘æ–°åˆ†æ”¯ï¼Œä¼˜åŒ–å€™é€‰è§£å†³æ–¹æ¡ˆ",
        "tools": "\nğŸ”§ **è°ƒç”¨ä¸“ä¸šå·¥å…·**\n\nâš™ï¸ æ‰§è¡Œå¿…è¦çš„å·¥å…·æ“ä½œè·å–ä¿¡æ¯",
    }

    # æ›´æ–°çŠ¶æ€
    if node_name == "generate_initial_response":
        lats_state['has_initial_response'] = True
    elif node_name == "expand":
        lats_state['is_searching'] = True

    # å‘é€èŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯
    node_message = node_messages.get(node_name)
    if node_message and node_message not in sent_contents:
        await _send_sse_data(res, chat_id, created, model, node_message)
        sent_contents.add(node_message)
        await asyncio.sleep(0.3)


async def _process_node_message(
    message: Any,
    current_node_type: str,
    lats_state: Dict[str, Any],
    iteration_counter: int
) -> Optional[str]:
    """
    åŸºäºèŠ‚ç‚¹ç±»å‹å’ŒçŠ¶æ€å¤„ç†æ¶ˆæ¯
    å‡å°‘ä¸å¿…è¦çš„æ—¥å¿—è¾“å‡º
    """
    try:
        if not hasattr(message, 'content') or not message.content:
            return None

        message_type = type(message).__name__
        content = message.content.strip()

        if not content:
            return None

        # åŸºäºèŠ‚ç‚¹ç±»å‹å¤„ç†
        if current_node_type == "generate_initial_response":
            return _handle_initial_response_message(message_type, content, lats_state)

        elif current_node_type == "expand":
            return _handle_expand_message(message_type, content, lats_state, iteration_counter)

        elif current_node_type == "tools":
            return _handle_tools_message(message_type, content, lats_state)

        # å¯¹äºå…¶ä»–èŠ‚ç‚¹æˆ–æ— æ˜ç¡®èŠ‚ç‚¹çš„æƒ…å†µ
        else:
            return _handle_general_message(message_type, content, lats_state)

    except Exception as e:
        logger.error(f"[LATS SSE] å¤„ç†èŠ‚ç‚¹æ¶ˆæ¯å¤±è´¥: {str(e)}")
        return None


def _handle_initial_response_message(message_type: str, content: str, lats_state: Dict[str, Any]) -> Optional[str]:
    """å¤„ç†åˆå§‹å“åº”èŠ‚ç‚¹çš„æ¶ˆæ¯"""
    if "ToolMessage" in message_type:
        return "\nğŸ”§ **åˆå§‹ä¿¡æ¯æ”¶é›†å®Œæˆ**\n\nğŸ“Š æ­£åœ¨åˆ†æè·å–çš„ä¿¡æ¯...\n"

    elif "AIMessage" in message_type or "AIMessageChunk" in message_type:
        # åˆå§‹å“åº”é˜¶æ®µçš„AIæ¶ˆæ¯
        if _is_substantial_response(content):
            lats_state['has_initial_response'] = True
            return f"\nğŸ’¡ **åˆå§‹è§£å†³æ–¹æ¡ˆ**\n\n{content}\n\n"
        elif len(content) > 30:
            return f"\nğŸ’¡ **åˆå§‹åˆ†æ**\n\n{content}\n\n"

    return None


def _handle_expand_message(message_type: str, content: str, lats_state: Dict[str, Any], iteration_counter: int) -> Optional[str]:
    """å¤„ç†æ‰©å±•æœç´¢èŠ‚ç‚¹çš„æ¶ˆæ¯"""
    if "ToolMessage" in message_type:
        return f"\nğŸ”§ **æœç´¢è¿­ä»£ #{iteration_counter} å·¥å…·è°ƒç”¨å®Œæˆ**\n\nğŸ“‹ è·å–åˆ°æ–°ä¿¡æ¯ï¼Œç»§ç»­å€™é€‰æ–¹æ¡ˆè¯„ä¼°...\n"

    elif "AIMessage" in message_type or "AIMessageChunk" in message_type:
        # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ ¼å¼çš„å†…éƒ¨è¯„ä¼°æ•°æ®
        if _is_internal_evaluation_data(content):
            # æ˜¾ç¤ºè¯„ä¼°è¿‡ç¨‹ï¼ˆä½†ä¸æ˜¾ç¤ºåŸå§‹JSONï¼‰
            if not lats_state.get('evaluation_shown', False):
                lats_state['evaluation_shown'] = True
                return f"\nğŸ“Š **å€™é€‰æ–¹æ¡ˆè¯„ä¼°**\n\nğŸ¤” æ­£åœ¨åˆ†æç¬¬ {iteration_counter} è½®æœç´¢ç»“æœçš„è´¨é‡å’Œå¯è¡Œæ€§...\n"
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„å€™é€‰æ–¹æ¡ˆï¼ˆä¸å†ä½¿ç”¨å†…å®¹åˆ†æåˆ¤æ–­æœ€ç»ˆç­”æ¡ˆï¼‰
        elif _is_substantial_response(content):
            return f"\n\nğŸ’¡ **å€™é€‰æ–¹æ¡ˆç”Ÿæˆ**\n\næ­£åœ¨è¯„ä¼°æ–°çš„è§£å†³æ–¹æ¡ˆ...\n\n"

    return None


def _handle_tools_message(message_type: str, content: str, lats_state: Dict[str, Any]) -> Optional[str]:
    """å¤„ç†å·¥å…·è°ƒç”¨èŠ‚ç‚¹çš„æ¶ˆæ¯"""
    if "ToolMessage" in message_type:
        if len(content) > 500:
            return "\nğŸ”§ **å·¥å…·æ‰§è¡Œå®Œæˆ**\n\nğŸ“Š å·²è·å–è¯¦ç»†ä¿¡æ¯ï¼Œæ­£åœ¨æ•´ç†åˆ†æ...\n"
        else:
            return "\nğŸ”§ **å·¥å…·æ‰§è¡Œå®Œæˆ**\n\nğŸ“‹ å·²è·å–ç›¸å…³ä¿¡æ¯\n"

    elif "AIMessage" in message_type and _is_substantial_response(content):
        return f"\nğŸ”§ **å·¥å…·å¤„ç†ç»“æœ**\n\n{content}\n\n"

    return None


def _handle_general_message(message_type: str, content: str, lats_state: Dict[str, Any]) -> Optional[str]:
    """å¤„ç†ä¸€èˆ¬æ¶ˆæ¯"""
    if "AIMessage" in message_type:
        # è¿‡æ»¤å†…éƒ¨è¯„ä¼°æ•°æ®
        if _is_internal_evaluation_data(content):
            return None

        # ä¸å†åœ¨è¿™é‡Œåˆ¤æ–­æœ€ç»ˆç­”æ¡ˆï¼Œç­‰å¾…æ­£å¼çš„çŠ¶æ€è¾“å‡º
        if _is_substantial_response(content):
            return f"\n\nğŸ’¬ **å›ç­”ç‰‡æ®µ**\n\n{content}\n\n"

    return None


def _is_internal_evaluation_data(content: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯å†…éƒ¨è¯„ä¼°æ•°æ®ï¼ˆJSONæ ¼å¼çš„è¯„åˆ†ã€åæ€ç­‰ï¼‰"""
    content_stripped = content.strip()

    # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ ¼å¼
    if (content_stripped.startswith('{') and content_stripped.endswith('}')) or \
       (content_stripped.startswith('[') and content_stripped.endswith(']')):

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯„ä¼°ç›¸å…³çš„å­—æ®µ
        evaluation_fields = ["reflections",
                             "score", "found_solution", "evaluation"]
        content_lower = content.lower()

        if any(field in content_lower for field in evaluation_fields):
            return True

    return False


def _is_substantial_response(content: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰å®è´¨å†…å®¹çš„å›ç­”"""
    if len(content) < 10:
        return False

    # æ’é™¤æ˜æ˜¾çš„ç³»ç»Ÿæ¶ˆæ¯æˆ–æç¤º
    system_indicators = [
        content.startswith('System:'),
        content.startswith('Human:'),
        content.startswith('Assistant:'),
        'tool_call' in content.lower(),
        'function_call' in content.lower(),
        content.strip().endswith('...'),  # çœç•¥å·ç»“å°¾é€šå¸¸æ˜¯ä¸å®Œæ•´å†…å®¹
        len(content.strip().split()) < 3,  # è¯æ±‡å¤ªå°‘
    ]

    return not any(system_indicators)


async def _send_sse_data(res, chat_id: str, created: int, model: str, content: str, finish_reason: str = None) -> None:
    """å‘é€SSEæ•°æ®"""
    response = {
        "id": chat_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model,
        "choices": [{
            "delta": {
                "role": "assistant",
                "content": content
            },
            "index": 0,
            "finish_reason": finish_reason
        }]
    }

    json_str = json.dumps(response, ensure_ascii=False, separators=(',', ':'))
    await res.write(f"data: {json_str}\n\n".encode('utf-8'))


async def _send_end_signal(res, chat_id: str, created: int, model: str) -> None:
    """å‘é€ç»“æŸä¿¡å·"""
    end_response = {
        "id": chat_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model,
        "choices": [{
            "delta": {},
            "index": 0,
            "finish_reason": "stop"
        }]
    }

    json_str = json.dumps(
        end_response, ensure_ascii=False, separators=(',', ':'))
    await res.write(f"data: {json_str}\n\n".encode('utf-8'))
    await res.write("data: [DONE]\n\n".encode('utf-8'))


async def _adaptive_delay(content: str) -> None:
    """æ ¹æ®å†…å®¹ç±»å‹è‡ªé€‚åº”å»¶è¿Ÿ"""
    if "æœç´¢è¿­ä»£" in content or "å€™é€‰è§£å†³æ–¹æ¡ˆ" in content:
        await asyncio.sleep(0.4)
    elif "å·¥å…·" in content or "è°ƒç”¨" in content:
        await asyncio.sleep(0.3)
    elif "æœ€ç»ˆç­”æ¡ˆ" in content or "è§£å†³æ–¹æ¡ˆ" in content:
        await asyncio.sleep(0.2)
    else:
        await asyncio.sleep(0.1)
