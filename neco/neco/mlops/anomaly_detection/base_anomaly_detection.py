import abc
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from loguru import logger
from hyperopt import fmin, tpe, Trials, space_eval
from sklearn.metrics import (
    precision_recall_fscore_support,
    roc_auc_score,
    accuracy_score
)

from neco.mlops.anomaly_detection.feature_engineer import TimeSeriesFeatureEngineer
from neco.mlops.utils.mlflow_utils import MLFlowUtils
from neco.mlops.utils.ml_utils import MLUtils


class BaseAnomalyDetection(abc.ABC):
    """å¼‚å¸¸æ£€æµ‹åŸºç±»ï¼Œæä¾›é€šç”¨çš„è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½"""

    def __init__(self):
        self.feature_engineer = None

    @abc.abstractmethod
    def build_model(self, train_params: dict):
        """æ„å»ºæ¨¡å‹å®ä¾‹"""
        pass

    def preprocess(self, df: pd.DataFrame, frequency: Optional[str] = None) -> Tuple[pd.DataFrame, List[str], Optional[str]]:
        """æ•°æ®é¢„å¤„ç†ï¼šæ—¶é—´æ ‡å‡†åŒ–ã€æ’åºã€ç¼ºå¤±å€¼å¡«å……"""
        if df is None:
            return None, [], frequency

        df = df.copy()

        # æ ‡å‡†åŒ–æ—¶é—´åˆ—å¹¶æ’åº
        if not np.issubdtype(df["timestamp"].dtype, np.datetime64):
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df = df.dropna(subset=["timestamp"]).sort_values("timestamp")

        # è®¾ç½®æ—¶é—´ç´¢å¼•ï¼Œæ¨æ–­é¢‘ç‡
        df = df.set_index("timestamp")
        if frequency is None:
            try:
                frequency = pd.infer_freq(df.index)
            except Exception as e:
                logger.warning(f"æ— æ³•æ¨æ–­æ—¶é—´é¢‘ç‡: {e}")
                frequency = None

        # å¤„ç†ç¼ºå¤±å€¼ï¼šæ—¶é—´æ’å€¼ -> å‰åå¡«å…… -> ä¸­ä½æ•°å…œåº•
        value_series = df["value"].astype(float)
        value_series = value_series.interpolate(method="time", limit_direction="both")
        value_series = value_series.ffill().bfill()

        if value_series.isna().any():
            median_value = value_series.median() if not np.isnan(value_series.median()) else 0.0
            value_series = value_series.fillna(median_value)

        df["value"] = value_series
        df = df.reset_index()

        return df, ["value"], frequency

    def predict(
        self,
        data: pd.DataFrame,
        model_name: str,
        model_version: str = "latest",
        threshold: float = 0.5
    ) -> pd.DataFrame:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå¼‚å¸¸æ£€æµ‹é¢„æµ‹"""
        # åŠ è½½æ¨¡å‹
        model = MLFlowUtils.load_model(model_name, model_version)
        
        # æ•°æ®é¢„å¤„ç†
        test_df, _, _ = self.preprocess(data, getattr(model, 'frequency', None))

        # ç‰¹å¾æå–
        feature_engineer = TimeSeriesFeatureEngineer(tsfresh_params=None, n_jobs=4)
        X_test, _, _ = feature_engineer.extract_features(
            test_df,
            selected_features=model.feature_cols,
            extract_labels=False
        )
        
        # é¢„æµ‹
        anomaly_scores = MLUtils.get_prediction_scores(model, X_test)
        anomaly_labels = MLUtils.apply_threshold(anomaly_scores, threshold)

        return pd.DataFrame({
            'timestamp': test_df['timestamp'],
            'value': test_df['value'],
            'anomaly_probability': anomaly_scores,
            'anomaly_label': anomaly_labels
        })

    def train(
        self,
        model_name: str,
        train_dataframe: pd.DataFrame,
        val_dataframe: Optional[pd.DataFrame] = None,
        test_dataframe: Optional[pd.DataFrame] = None,
        train_config: dict = {},
        max_evals: int = 50,
        mlflow_tracking_url: Optional[str] = None,
        experiment_name: str = "Default",
        tsfresh_params: Optional[Dict] = None,
        n_jobs: int = 0,
        primary_metric: str = "f1",
        positive_label: int = 1,
        decision_threshold: float = 0.5
    ):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        MLFlowUtils.setup_experiment(mlflow_tracking_url, experiment_name)

        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        self.feature_engineer = TimeSeriesFeatureEngineer(
            tsfresh_params=tsfresh_params,
            n_jobs=n_jobs
        )

        # æ•°æ®é¢„å¤„ç†
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        train_df_prep, _, frequency = self.preprocess(train_dataframe, None)
        val_df_prep = self.preprocess(val_dataframe, frequency)[0]
        test_df_prep = self.preprocess(test_dataframe, frequency)[0]

        # ç‰¹å¾å·¥ç¨‹
        logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        X_train, y_train, feature_cols = self.feature_engineer.extract_features(train_df_prep)
        
        # æ‰“å°ç‰¹å¾ä¿¡æ¯
        logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(feature_cols)} ä¸ªæœ‰æ•ˆç‰¹å¾")
        logger.info(f"ç‰¹å¾åç§°åˆ—è¡¨: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}")
        
        # å‡†å¤‡éªŒè¯é›†
        logger.info("å¼€å§‹éªŒè¯é›†ç‰¹å¾å·¥ç¨‹...")
        X_val, y_val, _ = self.feature_engineer.extract_features(
            val_df_prep, selected_features=feature_cols
        )

        # è¶…å‚æ•°ä¼˜åŒ–
        logger.info(f"ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œæœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evals}")
        
        space = MLUtils.build_search_space(train_config)
        trials = Trials()
        train_scores_history = []
        val_scores_history = []
        
        def objective(params_raw):
            params = space_eval(space, params_raw)
            try:
                model = self.build_model(train_params=params)
                model.fit(X_train, y_train)

                # è®¡ç®—éªŒè¯åˆ†æ•°
                val_scores = MLUtils.get_prediction_scores(model, X_val)
                val_score = MLUtils.calculate_metric_score(
                    y_val, val_scores, primary_metric, decision_threshold, positive_label
                )
                
                # è®°å½•å†å²
                if len(train_scores_history) % 10 == 0:
                    train_scores = MLUtils.get_prediction_scores(model, X_train)
                    train_score = MLUtils.calculate_metric_score(
                        y_train, train_scores, primary_metric, decision_threshold, positive_label
                    )
                    logger.info(f"ç¬¬ {len(train_scores_history)} æ¬¡è¯„ä¼° - è®­ç»ƒ{primary_metric}: {train_score:.4f}, éªŒè¯{primary_metric}: {val_score:.4f}")
                
                train_scores_history.append(0)  # å ä½ç¬¦
                val_scores_history.append(val_score)
                
                return {"loss": -float(val_score), "status": "ok"}
            except Exception as e:
                logger.error(f"è¶…å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
                return {"loss": 1.0, "status": "ok"}

        best_params_raw = fmin(
            fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals,
            trials=trials, rstate=np.random.default_rng(2025)
        )
        best_params = space_eval(space, best_params_raw)

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        logger.info("ğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        best_model = self.build_model(train_params=best_params)
        best_model.fit(X_train, y_train)

        # ä¿å­˜å…ƒæ•°æ®
        best_model.feature_cols = feature_cols
        best_model.frequency = frequency
        best_model.tsfresh_params = tsfresh_params

        # è¯„ä¼°æ¨¡å‹
        # éªŒè¯é›†è¯„ä¼°
        val_scores = MLUtils.get_prediction_scores(best_model, X_val)
        val_metrics = {"auc": float(roc_auc_score(y_val, val_scores))}

        # æµ‹è¯•é›†è¯„ä¼°
        X_test, y_test, _ = self.feature_engineer.extract_features(
            test_df_prep, selected_features=feature_cols
        )
        test_scores = MLUtils.get_prediction_scores(best_model, X_test)
        y_test_pred = MLUtils.apply_threshold(test_scores, decision_threshold)
        
        P, R, F1, _ = precision_recall_fscore_support(
            y_test, y_test_pred, pos_label=positive_label,
            average="binary", zero_division=0
        )
        
        test_metrics = {
            "auc": float(roc_auc_score(y_test, test_scores)),
            "precision": float(P),
            "recall": float(R), 
            "f1": float(F1),
            "accuracy": float(accuracy_score(y_test, y_test_pred))
        }

        # è®°å½•åˆ°MLflow
        all_params = {
            **best_params,
            "max_evals": max_evals,
            "primary_metric": primary_metric,
            "decision_threshold": decision_threshold,
            "n_features": len(feature_cols),
            "train_samples": len(X_train)
        }
        
        MLFlowUtils.log_training_results(
            params=all_params,
            train_scores=train_scores_history,
            val_scores=val_scores_history,
            metric_name=primary_metric,
            val_metrics=val_metrics,
            test_metrics=test_metrics,
            model=best_model,
            model_name=model_name
        )

        logger.info(f"âœ… æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ")

        return {
            "best_params": best_params,
            "best_model": best_model,
            "feature_cols": feature_cols,
            "val_metrics": val_metrics,
            "test_metrics": test_metrics,
            "trials": trials
        }
